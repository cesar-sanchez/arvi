\documentclass{beamer}

\usepackage{multicol}
\usepackage{color}
\newcommand{\addhere}{{\color{red} Anything Else?}}
\usepackage{url}

\usetheme{Singapore}
\AtBeginSection{\frame{\sectionpage}}

\title[WG3]{Working Group 3\\
{\bf Challenging Computational Domains}}
\author{Chair: Keiko Nakata\\ Vice-chair: Cesar Sanchez}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents  
\end{frame}

%%%%%%%%%%%%%%
\section{MoU and WG3}
%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Main Objective of the Action (from the MoU)}

  \begin{block}{Memorandum of Understanding}
    The main objective of the Action is to consolidate a network of
    runtime verification experts and practitioners in application
    domains, so that they jointly find new principles for reliable
    system engineering using monitoring as a building block.
  \end{block}
  
  \pause
  \vspace{2em}

  \begin{block}{A. Abstract and Keywords}
  The main goal is to overcome the fragmentation of RV research by:
  (1) the design of common input formats for tool cooperation and
    comparison;
  (2) the evaluation of different tools, building a growing sets
    benchmarks and running tool competitions; and
  (3) \alert{by designing a road-map and grand challenges extracted from
    application domains.}
\end{block}

\end{frame}


\begin{frame}
  \frametitle{Main Objective of the Action (from the MoU) (2)}

\begin{block}{C.2 Objectives}
\small
To achieve the overall aim, the main objectives are:
\begin{itemize}
\item the development of a common infrastructure that enables the
  development of a collection of runtime verification problems and
  benchmarks for the comparison of algorithms and tools, and to
  increase their collaboration
\item \alert{the development and sharing of current challenges in runtime
  verification and monitoring}
\item the development of an interaction between the runtime
  verification community of experts at large with practitioners from
  application domains that could benefit from this technology, and
  influence its developments
\item education of young researchers and potential users of monitoring technologies
\item coordination of European research on monitoring, runtime
  verification and its applications
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{MoU (3)}
\begin{block}{C.4 Potential impact of the Action}
  This Action will coordinate the European efforts in the field of
  runtime verification and applications.\\[1em]

  [...]\\[1em]

  Concrete outcomes of the Action will include (1) a taxonomy of
  problems and techniques, and a taxonomy of existing tools, (2) a
  common family of input languages for describing problems and
  solutions, (3) a collection of benchmarks that allows to compare the
  different tools, (4) \alert{a set of challenges for the
    applicability of runtime verification to important areas of
    application.}
\end{block}
\vfill

\end{frame}

\begin{frame}
\frametitle{MoU (4). D.1 Scientific Focus}

\begin{block}{Challenging computation domains}
  Runtime verification has been studied in the context of several
  domains including embedded systems, hardware, distributed systems,
  and unreliable/approximate domains. To date such studies are
  fragmented and scattered at best. Through the Action, the existing
  work will be reviewed and a roadmap for future work will be drawn
  up.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{WG3. Aim and Objectives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} 
  \frametitle{WG3 Aim}
  \Large
  
  \textit{To study novel and challenging computational domains for
    runtime verification and monitoring that result from the study of
    other application areas than programming languages.}
\end{frame}

\begin{frame} 
  \frametitle{WG3 Objectives}
  
  From MoU: \textbf{D.2 Scientific work plan methods and means} \\[2em]
  
  The objectives of this Working Group will be to identify the
  challenges for monitoring in the following application domains:
  \begin{itemize}
  \item \alert{Distributed systems}, where the timing of observations may vary
    widely in a non- synchronized manner.
  \item \alert{Embedded systems}, where the resources of the monitor
    are constrained.
  \item \alert{Hardware}, where the timing must be precise and the
    monitor must operate non disruptively.
  \item \alert{Unreliable} domains and \alert{approximated} domains,
    where either the system is not reliable, or aggregation or
    sampling is necessary due to large amounts of data.  
  \end{itemize}
  These areas involve expertise from more than one domain and have a
  much higher chance of success if attacked cooperatively.
\end{frame}

\begin{frame}
  \frametitle{WG3 Output}
  From MoU: \textbf{D.2 Scientific work plan methods and means}\\[2em]

  The \alert{concrete outputs} of WG3:
  \begin{itemize}
  \item First, a series of documents will be worked out giving a
    \alert{roadmap} for the application of runtime verification techniques to
    the areas listed above, identifying connections with established
    work in the respective sub-areas of computer science, and
    challenges and opportunities. 
  \item Second, a \alert{concrete case study} will performed, in which
    a runtime verification solution for multicore systems will be
    developed using dedicated monitoring hardware based on FPGAs to
    show the feasibility and general applicability of runtime
    verification techniques.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{WG3 Milestones}

  From MoU: \textbf{E.1 Coordination and organisation}\\[2em]

  \textbf{Milestones}\\[2em]
  
\begin{itemize}
\item M1 (end of Year 1): First Annual Report of the Action. Proposals
  on taxonomy, infrastructure, \alert{challenges}. [...]
\item M2 (end of Year 2): Second Annual Report of the Action.
  \alert{First document with challenges}. [...]
\item M3 (end of Year 3): Third Annual Report of the Action. First
  Summer school. [...]
\item M4 (end of Year 4): Final report of the Action. \alert{Published
    surveys}. Second Summer School. [...] Report on case studies using
  monitoring hardware and application domains, including medical
  devices.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%
\section{Organization}
%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Lines/ Domains}

  \begin{itemize}
  \item \alert{Distributed systems}: 
    Goal: how to monitor global properties from (local) observations. 
    \begin{itemize} 
    \item Theory: study what is monitorable. How much communication?
      Extra communication?
    \item Theroy: develop a general framework (logics, algorithsm).
    \item Practice: identify practical case studies.
    \item Practice: itentify useful fragments of the framework.
    \end{itemize}
  \item \alert{Embedded systems}, where resources are constrained.
    \begin{itemize} 
    \item Theory: study how to incorporate resources in monitors.
    \item How to instrument an embedded system.
    \item When to sampling a continuous signal (hybrid system).
    \end{itemize}
  \item \alert{Hardware}: two themes (``monitoring hardware'' and ``hardware
    for monitoring''). Issues:
    \begin{itemize}
    \item Precise timing.
    \item Non-intrusive monitoring.
    \item How much hardware is needed?
    \end{itemize}
  \item \alert{Unreliable} domains and \alert{approximated} domains.
    \begin{itemize}
      \item E.g: monitoring dynamic large systems by sampling.
      \item E.g: what if the observation is imperfect (value, timing)?
      \item How to characterize imprecision?
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Line: Distributed Systems}

  \begin{itemize}
  \item \alert{Distributed systems}: 
    Goal: how to monitor global properties from (local) observations. 
    \begin{itemize} 
    \item Theory: study what is monitorable. How much communication?
      Extra communication?
    \item Theroy: develop a general framework (logics, algorithsm).
    \item Practice: identify practical case studies.
    \item Practice: itentify useful fragments of the framework.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Line: Embedded Systems} (CPSs)
  \begin{itemize}
  \item \alert{Embedded systems}, where resources are constrained.
    \begin{itemize} 
    \item Theory: study how to incorporate resources in monitors.
    \item How to instrument an embedded system.
    \item When to sampling a continuous signal (hybrid system).
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Lines: Hardware}

  \begin{itemize}
  \item \alert{Hardware}: two themes (``monitoring hardware'' and ``hardware
    for monitoring''). Issues:
    \begin{itemize}
    \item Precise timing.
    \item Non-intrusive monitoring.
    \item How much hardware is needed?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Lines: Unreliable domain}

  \begin{itemize}
  \item \alert{Unreliable} domains and \alert{approximated} monitoring.
    \begin{itemize}
      \item E.g: monitoring dynamic or large systems by sampling.
      \item E.g: what if the observation is imperfect (value, timing)?
      \item How to characterize imprecision?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Lines}

  Perhaps a common theme:\\[1em]

  \textit{Static analysis/information about the system is always
    exploitable for monitoring (at least for efficiency)}\\[1em]

  but\\[1em]

  \textit{but it (static information) maybe crucial for the monitorability of 
    challenging domains }
\end{frame}


\begin{frame}
  \frametitle{One Organizational Option}

  Create task-forces for \alert{each line} to cooperatively create a
  quick survey on the concrete state-of-the-art, including:
  \begin{itemize}
    \item publications (from action members and others)
    \item tools, intended applications
    \item current limitations
    \item related areas of CS, related research groups
    \item (speculate) on unknow results
  \end{itemize}

  These surveys can be the basis for:
  \begin{itemize}
  \item discussions, 
  \item shaping challenging problems 
  \item identifying potential applications,
  \item itentifying collaborations
  \end{itemize}
    
\end{frame}

\begin{frame}
  \frametitle{Embedded Debugging}


When an embedded software is tested in the deployed platform, the
engineers cannot use an interactive debugger to debug errors. 

An alternative is to produce logs to later analyze them.
\begin{itemize} 
\item the modifications in the program must be minimal (to prevent changes in
the timings) 
\item the inspection of traces is tedious
\item the logging
maybe unfeasible because large storages are typically not available. 
\end{itemize}

To offer the debugging engineer a language to
express interesting error trace descriptions (an RV language) which
must then be used to modify the code for which static analysis is
crucial to provide efficiency. 

There is an evidence that
instrumented code uffered a slowdown of orders of magnitude, and
then code was statically analyzed to alleviate this slowdown
significantly 

\end{frame}


\begin{frame}
  \frametitle{Concurrency}

- Coordination (wait/notify) is least explored area. (No tools, even research tools.)

- Which properties can you observe locally? 

- Deadlock detection cannot be monitored locally?

- Prove statically that some global property follows from a bunch of local properties (assumptions), and you can monitor those local properties at runtime.

- How do you learn a context-specific behavior of a component in a disributed system?

- Determinism can be more abstract and expressive than absence of deadlocks.

- Use rely-guarantee techniques at runtime to make monitoring more composition

\end{frame}

\begin{frame}
  \frametitle{Distributed systems}

- Modularity, partial static analysis, updates on components.

- Contracts: with run-time checks we can have more permissive contracts
  between components.

- Analyze contracts incrementally.

- Need to constrain design of systems to provide guarantees.

- Current systems produce much data/logs but still forencics cannot find
  root cause of some failures.

- Use static analysis to enhance effectiveness of monitors.

- Need specific assumptions on system design/implementation for verification.

- Challenge: distributed systems, each component has a monitor but monitor
  is not reliable. How we can identify misbehaving processes?

\end{frame}

\end{document} 
